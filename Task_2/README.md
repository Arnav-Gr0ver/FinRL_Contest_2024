# FinRL task 2
In this task you will need to finetune an LLM to produce daily trading signals for the sp500 based on current news. The quality of the trading signals will be tested with a fixed time exit strategy that can either do nothing, long or short a given stock. You may use the trajectories generated by the env to finetune your LLM, as well as using sentiment datasets to improve the quality of predictions on the news. 

## Rules
You may use any external datasets in your finetuning, which should be done on Llama 3 8B instruct **subject to change**. We will be testing your models on news and stock movements after the submission deadline to ensure no data leakage.

You may use any additional stock data for training, however we will use only the ten tickers shown in tas2_sample.py as well as news relating to these tickers from the polygon api. Participants are not required to use the news api that we are using, nor are you required to rely on yfinance to fetch data on stocks. 

## Evaluation and Submission guidelines
Please submit your model and appropriate tokenizer, as well as any files that are needed to run your submission. Evaluation will be done by importing your submissions into a file similar to task2_sample.py to execute all submissions on an identical platform. We ask partiicpants to therefure submit all of their relevant materials whilst maintaining the endpoints in the other task2 files to facilitate testing.

Please provide a readme that describes your submission and explains important things to note when running it so that we can ensure we run your submission as it was intended.

## Starter kit description
The starter kit works as follows: 
- task2_sample.py : this file imports a model and tokenizer. It calls helper functions from the other task2_ files to fetch stock tickers, fetch appropriate news to tickers and generate sentiment using the model and tokenizer. 
- task2_<function> : these files include helper code endpoints which we will be calling in evaluation. You are free to change anything in these files, as long the function imports are not broken. 
- environment : we provide an environment for this competition which defines an observation and action space for the LLM agent to explore. Moreover this env generates reward signals using a simple fixed time exit strategy which you can use to finetune your agent. You are free to make changes to the RL setup of the submission for example by adding critic models, or by modifying the reward settings as well as the buy and sell thresholds and the sentiment outputs of your model. 


